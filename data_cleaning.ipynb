{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set new cache directories\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/ephemeral/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/ephemeral/transformers_cache\"\n",
    "os.environ[\"TMPDIR\"] = \"/ephemeral/tmp\"\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(\"/ephemeral/hf_cache\", exist_ok=True)\n",
    "os.makedirs(\"/ephemeral/transformers_cache\", exist_ok=True)\n",
    "os.makedirs(\"/ephemeral/tmp\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load and save the dataset into the /ephemeral directory\n",
    "dataset = load_dataset(\"cc100\", \"mn\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text'],\n",
       "        num_rows: 15098167\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Load the CC100 dataset for Mongolian\n",
    "dataset = load_dataset(\"cc100\", \"mn\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# Define a function to clean Mongolian text\n",
    "def clean_mongolian_text(text):\n",
    "    \"\"\"\n",
    "    Cleans Mongolian text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing URLs\n",
    "    3. Removing HTML tags\n",
    "    4. Removing non-Mongolian characters\n",
    "    5. Normalizing whitespace\n",
    "    6. Removing short or meaningless lines\n",
    "    \"\"\"\n",
    "    if not text:  # Handle empty or None input\n",
    "        return None\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Remove non-Mongolian characters (retain Cyrillic, spaces, punctuation)\n",
    "    mongolian_pattern = r\"[А-Яа-яҮүӨөЁёЭэ0-9\\s.,!?;:\\\"'()-]\"\n",
    "    text = \"\".join([char if re.match(mongolian_pattern, char) else \" \" for char in text])\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Remove excessive punctuation (optional)\n",
    "    text = re.sub(r\"[!?]{2,}\", \"!\", text)  # Replace multiple exclamation marks\n",
    "    text = re.sub(r\"[.]{2,}\", \".\", text)   # Replace multiple periods with a single one\n",
    "\n",
    "    # Filter out very short lines\n",
    "    if len(text) < 20:  # Adjust this threshold as needed\n",
    "        return None\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply cleaning safely\n",
    "def clean_dataset(example):\n",
    "    \"\"\"\n",
    "    Cleans text and ensures only valid rows are processed.\n",
    "    \"\"\"\n",
    "    cleaned_text = clean_mongolian_text(example[\"text\"])\n",
    "    # Return the cleaned text if valid, otherwise keep the row unchanged\n",
    "    return {\"text\": cleaned_text} if cleaned_text else {\"text\": \"\"}\n",
    "\n",
    "# Remove invalid rows before mapping\n",
    "filtered_dataset = dataset.filter(lambda x: clean_mongolian_text(x[\"text\"]) is not None, num_proc=27)\n",
    "\n",
    "# Apply cleaning and update the text\n",
    "cleaned_dataset = filtered_dataset.map(clean_dataset, num_proc=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'id' column from the cleaned dataset\n",
    "dataset = cleaned_dataset.remove_columns(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids'],\n",
       "    num_rows: 11585734\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sequences_with_text(dataset, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Merge sequences in a dataset while keeping 'text' and 'input_ids' aligned.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset with 'text' and 'input_ids'.\n",
    "        tokenizer: Tokenizer used to tokenize the text.\n",
    "        max_length (int): Maximum length for merged sequences.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with merged 'text' and 'input_ids'.\n",
    "    \"\"\"\n",
    "    merged_texts = []\n",
    "    merged_input_ids = []\n",
    "    current_text = []\n",
    "    current_input_ids = []\n",
    "\n",
    "    for text, input_ids in zip(dataset[\"text\"], dataset[\"input_ids\"]):\n",
    "        # Check if adding the current sequence exceeds the max length\n",
    "        if len(current_input_ids) + len(input_ids) > max_length:\n",
    "            # Append the merged sequences\n",
    "            merged_texts.append(\" \".join(current_text))\n",
    "            merged_input_ids.append(current_input_ids[:max_length])\n",
    "            # Reset for the next sequence\n",
    "            current_text = []\n",
    "            current_input_ids = []\n",
    "        \n",
    "        # Extend the current sequence\n",
    "        current_text.append(text)\n",
    "        current_input_ids.extend(input_ids)\n",
    "    \n",
    "    # Add the final batch if it exists\n",
    "    if current_input_ids:\n",
    "        merged_texts.append(\" \".join(current_text))\n",
    "        merged_input_ids.append(current_input_ids[:max_length])\n",
    "\n",
    "    return {\"text\": merged_texts, \"input_ids\": merged_input_ids}\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Billyyy/bloomz_mn\")\n",
    "\n",
    "# Add 'input_ids' column if not already in the dataset\n",
    "if \"input_ids\" not in dataset.column_names:\n",
    "    dataset = dataset.map(\n",
    "        lambda examples: {\"input_ids\": tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=1024)[\"input_ids\"]},\n",
    "        batched=True,\n",
    "        num_proc=27\n",
    "    )\n",
    "\n",
    "# Apply the merging function\n",
    "merged_dataset = dataset.map(\n",
    "    lambda batch: merge_sequences_with_text(batch, tokenizer, max_length=1024),\n",
    "    batched=True, \n",
    "    num_proc=27\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (14/14 shards): 100%|██████████| 997125/997125 [00:06<00:00, 145578.37 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 35.68ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 34.24ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 34.59ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 32.57ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 29.96ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 31.90ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 34.70ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 35.62ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 35.07ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 34.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:01<00:00, 36.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 34.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 32.96ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 72/72 [00:02<00:00, 34.71ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 14/14 [01:28<00:00,  6.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Billyyy/cleaned-mongolian-dataset/commit/e350efd88be98bc4237f6554f59a790c31c38963', commit_message='Upload dataset', commit_description='', oid='e350efd88be98bc4237f6554f59a790c31c38963', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Billyyy/cleaned-mongolian-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Billyyy/cleaned-mongolian-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Save the cleaned dataset to disk in Arrow format\n",
    "merged_dataset.save_to_disk(\"/ephemeral/cleaned_mongolian_dataset\")\n",
    "\n",
    "# Reload the dataset as a DatasetDict if it's not already\n",
    "dataset_dict = DatasetDict({\"train\": merged_dataset})\n",
    "\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(\"Billyyy/cleaned-mongolian-dataset\", private=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
