import os
import argparse
import json
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import sacrebleu
from tqdm import tqdm
from datasets import load_dataset
from nltk.translate.meteor_score import meteor_score
from peft import PeftModel
import random
import numpy as np
from nltk.tokenize import word_tokenize  # Import tokenizer
import nltk

nltk.download("punkt")
nltk.download("wordnet")
nltk.download('punkt_tab')

# Fix randomness for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Load the FLORES-200 dataset
dataset = load_dataset("facebook/flores", "eng_Latn-khk_Cyrl", split="devtest")

def disable_lora_layers(model, disable_type):
    """
    Disables LoRA adapters in a subset of decoder layers by setting LoRA weights to zero.

    :param model: The LoRA fine-tuned model (PEFT)
    :param disable_type: Which layers to disable LoRA for ('early', 'middle', 'late', or 'none')
    """
    num_decoder_layers = len(model.base_model.model.model.decoder.layers)  # Get total decoder layers

    if disable_type == "early":
        disable_range = range(0, num_decoder_layers // 3)  # First 1/3 of layers
    elif disable_type == "middle":
        disable_range = range(num_decoder_layers // 3, 2 * num_decoder_layers // 3)  # Middle 1/3
    elif disable_type == "late":
        disable_range = range(2 * num_decoder_layers // 3, num_decoder_layers)  # Last 1/3
    elif disable_type == "last_2":
        disable_range = range(num_decoder_layers - 2, num_decoder_layers)  # Only last 2 layers
    elif disable_type == "first_2":
        disable_range = range(0, 2)  # Only last 2 layers
    else:
        return  # 'none' -> Do nothing

    # üîπ Step 1: Disable LoRA in Selected Decoder Layers
    for layer_idx in disable_range:
        for attn_type in ["self_attn", "encoder_attn"]:  # Self-attention + Encoder-Decoder attention
            for proj in ["q_proj", "v_proj"]:  # Attention projections
                try:
                    attn_layer = getattr(model.base_model.model.model.decoder.layers[layer_idx], attn_type)
                    
                    # Zero out LoRA weights
                    with torch.no_grad():
                        getattr(attn_layer, proj).lora_A.default.weight.zero_()
                        getattr(attn_layer, proj).lora_B.default.weight.zero_()

                    print(f"‚ùå LoRA Disabled in {attn_type}.{proj} (Layer {layer_idx})")

                except AttributeError:
                    print(f"‚ö†Ô∏è LoRA adapter not found in Layer {layer_idx}, {attn_type}.{proj} (Skipping)")

    print(f"‚úÖ Disabled LoRA in {disable_type} decoder layers (Layers: {list(disable_range)})")



def translate_batch(args, model, tokenizer, texts, max_length, device):
    """
    Translate a batch of texts using the specified model and tokenizer.
    Ensures deterministic output by disabling sampling.
    """
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=max_length).to(device)
    if args.model_name: 
        with torch.no_grad():
            output_tokens = model.generate(
                **inputs,
                max_length=max_length,
                use_cache=True  # Enables faster decoding
            )
    else:
        with torch.no_grad():
            output_tokens = model.generate(
                **inputs,
                max_length=max_length,
                use_cache=True,  # Enables faster decoding
                forced_bos_token_id=256047
            )
    return tokenizer.batch_decode(output_tokens, skip_special_tokens=True)


def evaluate_model(args):
    """
    Evaluate the translation model on the provided dataset and compute BLEU, chrF++, and METEOR scores.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print("üöÄ Loading NLLB model...")
    base_model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-1.3B").to(device)
    tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-distilled-1.3B", src_lang=args.src_lang)

    if args.model_name:
        model = PeftModel.from_pretrained(base_model, args.model_name).to(device)
        if args.freeze_layers != "none":
            disable_lora_layers(model, args.freeze_layers)  # Apply layer-wise freezing
    else:
        model = base_model

    batch_size = args.batch_size
    num_samples = len(dataset)

    predictions = []
    references = []

    for i in tqdm(range(0, num_samples, batch_size), desc="Translating in Batches"):
        batch = dataset.select(range(i, min(i + batch_size, num_samples)))  # ‚úÖ Proper batch selection

        src_texts = batch["sentence_khk_Cyrl"]  # Extract source texts correctly
        ref_texts = batch["sentence_eng_Latn"]  # Extract references correctly

        translated_texts = translate_batch(args, model, tokenizer, src_texts, args.max_length, device)

        predictions.extend(translated_texts)
        references.extend([[ref] for ref in ref_texts])  # sacrebleu expects a list of lists

    # Compute BLEU score
    bleu = sacrebleu.corpus_bleu(predictions, references, tokenize="intl")
    print(f"üéØ BLEU Score: {bleu.score:.2f}")

    # Compute chrF++ score
    chrf = sacrebleu.corpus_chrf(predictions, references, beta=2)
    print(f"üìä chrF++ Score: {chrf.score:.2f}")
    
    # Compute METEOR score
    meteor_scores = [
        meteor_score([word_tokenize(ref[0])], word_tokenize(pred))  # Convert to tokenized lists
        for ref, pred in zip(references, predictions)
    ]

    avg_meteor_score = sum(meteor_scores) / len(meteor_scores)
    print(f"üìå METEOR Score: {avg_meteor_score:.4f}")

    results = {
        "Model Name": args.model_name,
        "Frozen Layers": args.freeze_layers,
        "BLEU": bleu.score,
        "chrF++": chrf.score,
        "METEOR": avg_meteor_score,
        "Predictions": predictions
    }

    output_file = "results/flores_results.json"
    
    if os.path.exists(output_file):
        with open(output_file, "r") as f:
            existing_data = json.load(f)
        existing_data.append(results)
    else:
        existing_data = [results]

    with open(output_file, "w") as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=4)
    
    print(f"üìÇ Results saved to {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Evaluate a translation model using the FLORES-200 dataset.")
    parser.add_argument("--model_name", type=str, help="Name or path of the fine-tuned LoRA model.")
    parser.add_argument("--src_lang", type=str, default="khk_Cyrl", help="Source language code (e.g., 'eng_Latn').")
    parser.add_argument("--max_length", type=int, default=256, help="Maximum length for the generated translations.")
    parser.add_argument("--batch_size", type=int, default=1, help="Batch size for translation speed optimization.")
    parser.add_argument(
        "--freeze_layers",
        type=str,
        choices=["none", "early", "middle", "late","last_2","first_2"],
        default="none",
        help="Freeze specific decoder layers: 'none' (default), 'early', 'middle', 'late'."
    )
    args = parser.parse_args()

    evaluate_model(args)


if __name__ == "__main__":
    main()
