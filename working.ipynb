{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set new cache directories\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/ephemeral/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/ephemeral/transformers_cache\"\n",
    "os.environ[\"TMPDIR\"] = \"/ephemeral/tmp\"\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(\"/ephemeral/hf_cache\", exist_ok=True)\n",
    "os.makedirs(\"/ephemeral/transformers_cache\", exist_ok=True)\n",
    "os.makedirs(\"/ephemeral/tmp\", exist_ok=True)\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,Trainer,TrainingArguments,DataCollatorForLanguageModeling,DataCollatorWithPadding\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): M2M100ForConditionalGeneration(\n",
       "      (model): M2M100Model(\n",
       "        (shared): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "        (encoder): M2M100Encoder(\n",
       "          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x M2M100EncoderLayer(\n",
       "              (self_attn): M2M100SdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): ReLU()\n",
       "              (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "              (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): M2M100Decoder(\n",
       "          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x M2M100DecoderLayer(\n",
       "              (self_attn): M2M100SdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): M2M100SdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "              (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=256206, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer with the source language (Mongolian in Cyrillic)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/nllb-200-distilled-1.3B\",\n",
    "    src_lang=\"khk_Cyrl\"\n",
    ")\n",
    "\n",
    "# Load the base model directly onto the CPU\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"facebook/nllb-200-distilled-1.3B\",\n",
    "    device_map={\"\": \"cpu\"}  # Explicitly set the device map to CPU\n",
    ")\n",
    "\n",
    "# Load the fine-tuned LoRA model onto the CPU\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"Billyyy/mon_nllb_3B\",\n",
    "    torch_device=\"cpu\"  # Ensure the adapter is loaded on the CPU\n",
    ")\n",
    "\n",
    "# Confirm the model is on CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the FLORES-200 dataset\n",
    "dataset = load_dataset(\"facebook/flores\", \"eng_Latn-khk_Cyrl\", split=\"devtest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5,\n",
       " 'URL': 'https://en.wikinews.org/wiki/Nobel_Prize_in_Literature_Committee_abandons_efforts_to_contact_Bob_Dylan',\n",
       " 'domain': 'wikinews',\n",
       " 'topic': 'music',\n",
       " 'has_image': 0,\n",
       " 'has_hyperlink': 0,\n",
       " 'sentence_eng_Latn': 'Danius said, \"Right now we are doing nothing. I have called and sent emails to his closest collaborator and received very friendly replies. For now, that is certainly enough.\"',\n",
       " 'sentence_khk_Cyrl': 'Даниус хэлэхдээ \"Яг одоо бид юу ч хийхгүй байгаа. Бид түүний хамгийн ойрын хамтрагчтай ярьж, цахим шуудан илгээсэн ба маш нөхөрсөг хариулт хүлээн авсан. Одоохондоо тэр л хангалттай\" гэв.'}"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"  # Adjust based on testing\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"8\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Question: бахамад ямар мөнгө авах вэ?\n",
      "Answer Key: A\n",
      "Choices:\n",
      "  A: багамын доллар\n",
      "  B: британийн фунт\n",
      "  C: евро\n",
      "  D: ам.доллар\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_questions(filename):\n",
    "    \"\"\"\n",
    "    Load multiple-choice questions from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the JSON file containing questions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a question.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        questions = json.load(file)\n",
    "    return questions\n",
    "\n",
    "def extract_questions_answers_choices(questions):\n",
    "    \"\"\"\n",
    "    Extract questions, their corresponding answer keys, and choices with labels.\n",
    "\n",
    "    Args:\n",
    "        questions (list): A list of dictionaries, each containing a question and its details.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Three lists - questions, answer keys, and choices with labels.\n",
    "    \"\"\"\n",
    "    question_texts = []\n",
    "    answer_keys = []\n",
    "    choices_with_labels = []\n",
    "\n",
    "    for q in questions:\n",
    "        question_texts.append(q['question'])\n",
    "        answer_keys.append(q['answerKey'])\n",
    "        choices = {choice['label']: choice['text'] for choice in q['choices']}\n",
    "        choices_with_labels.append(choices)\n",
    "\n",
    "    return question_texts, answer_keys, choices_with_labels\n",
    "\n",
    "# Path to your JSON file\n",
    "filename = 'evaluation/MM-Eval/knowledge_eval.json'\n",
    "\n",
    "# Load questions from the JSON file\n",
    "questions = load_questions(filename)\n",
    "\n",
    "# Extract questions, answer keys, and choices with labels\n",
    "question_texts, answer_keys, choices_with_labels = extract_questions_answers_choices(questions)\n",
    "\n",
    "# Example usage: print the first question, its answer key, and choices\n",
    "print(f\"First Question: {question_texts[0]}\")\n",
    "print(f\"Answer Key: {answer_keys[0]}\")\n",
    "print(\"Choices:\")\n",
    "for label, text in choices_with_labels[0].items():\n",
    "    print(f\"  {label}: {text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'D',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'B',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'C',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'D',\n",
       " 'D',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'D',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'D',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'D',\n",
       " 'B',\n",
       " 'D',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'D',\n",
       " 'B',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'C',\n",
       " 'D',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'D',\n",
       " 'C',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'C',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'B',\n",
       " 'D',\n",
       " 'C',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'C',\n",
       " 'C',\n",
       " 'D',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'D',\n",
       " 'A',\n",
       " 'D',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'D',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'D',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'D',\n",
       " 'B',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'C']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 15/1012 [01:34<1:45:01,  6.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m ref_text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_eng_Latn\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Mongolian reference\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Translate using the pre-trained model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Store for BLEU evaluation\u001b[39;00m\n\u001b[1;32m     23\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(translated_text)\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mtranslate_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     output_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/peft/peft_model.py:2109\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2107\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2108\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 2109\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:1598\u001b[0m, in \u001b[0;36mM2M100ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1577\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1578\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1579\u001b[0m         )\n\u001b[1;32m   1581\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1582\u001b[0m     input_ids,\n\u001b[1;32m   1583\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1597\u001b[0m )\n\u001b[0;32m-> 1598\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1600\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;66;03m# move labels to the correct device to enable PP\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "import tqdm\n",
    "\n",
    "def translate_text(text):\n",
    "    \"\"\"Translates English text to Mongolian using the pre-trained model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**inputs, max_length=256)\n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Collect model predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tqdm.tqdm(dataset):\n",
    "    src_text = example[\"sentence_khk_Cyrl\"]  # English input\n",
    "    ref_text = example[\"sentence_eng_Latn\"]  # Mongolian reference\n",
    "    \n",
    "    # Translate using the pre-trained model\n",
    "    translated_text = translate_text(src_text)\n",
    "    \n",
    "    # Store for BLEU evaluation\n",
    "    predictions.append(translated_text)\n",
    "    references.append([ref_text])  # sacrebleu expects a list of references\n",
    "\n",
    "# Compute BLEU score\n",
    "import sacrebleu\n",
    "bleu = sacrebleu.corpus_bleu(predictions, references, tokenize=\"intl\")\n",
    "print(f\"BLEU Score (Pre-trained NLLB-200): {bleu.score:.2f}\")\n",
    "\n",
    "# Compute chrF++ score\n",
    "chrf = sacrebleu.corpus_chrf(predictions, references, beta=2)  # chrF++ (β=2 for recall emphasis)\n",
    "print(f\"chrF++ Score (Pre-trained NLLB-200): {chrf.score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_first_lines_text(file_path, num_lines=5):\n",
    "    \"\"\"\n",
    "    Reads and prints the first `num_lines` lines from a plain text file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i in range(num_lines):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break  # Stop if file has fewer than num_lines\n",
    "            print(f\"Line {i+1}: {line.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "device = \"cuda\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Answer: You are a AI problem solver working with very bad quality questions. Read the problem carefully and answer based on the variation of question that makes more sense and output the answer in following format ///YOUR NUMBER/// the numerical answer, without explanation or reasoning.\n",
      "\n",
      "Problem: Janet's half-day egg is 16 eggs. He eats three in the morning and uses four daily for his friends to make muffins. The remaining eggs are sold at $ 2 per fresh egg in the farmer's market every day. How much money does he make every day at the farmer's market?\n",
      "\n",
      "Answer: 0\n",
      "\n",
      "Explanation: Janet's half-day egg is 16 eggs. He eats three in the morning and uses four daily for his friends to make muffins. The remaining eggs are sold at $2 per fresh egg in the farmer's market every day. So, 16 - 3 - 4 = 9 eggs left. 9 * 2 = $18. So, he makes $18 every day.\n",
      "\n",
      "Wait, but the problem says \"Janet's half-day egg is 16 eggs.\" That seems a bit confusing. Maybe it's a typo and should be \"Janet has 16 eggs for a half-day.\" So, she starts with 16 eggs. She eats 3 in the morning, uses 4 for her friends to make muffins, and then sells the remaining eggs at $2 each. So, 16 - 3 - 4 = 9 eggs. 9 * 2 = $18. So, she makes $18 every day.\n",
      "\n",
      "But the initial problem says \"Janet's half-day egg is 16 eggs.\" That phrasing is a bit unclear. Maybe it's supposed to say \"Janet has 16 eggs for a half-day.\" So, she uses them as described. So, the calculation is correct, and she makes $18 every day.\n",
      "\n",
      "But the user's initial answer was 0, which doesn't make sense. So, the correct answer should be $18.\n",
      "\n",
      "But the problem might have a different interpretation. Maybe \"half-day egg\" refers to something else, like she only has eggs for half a day, so she can't sell all of them. But that doesn't make much sense. It's more likely a typo or misphrasing.\n",
      "\n",
      "So, the correct calculation is 16 - 3 - 4 = 9 eggs sold at $2 each, totaling $18.\n",
      "\n",
      "But the initial answer was 0, which is incorrect. So, the correct answer is $18.\n",
      "</think>\n",
      "\n",
      "///18///\n"
     ]
    }
   ],
   "source": [
    "def get_numeric_answer(question):\n",
    "    \"\"\"Generate a response and extract only the number from DeepSeek.\"\"\"\n",
    "    prompt = f\"You are a AI problem solver working with very bad quality questions. Read the problem carefully and answer based on the variation of question that makes more sense and output the answer in following format ///YOUR NUMBER/// the numerical answer, without explanation or reasoning.\\n\\nProblem: {question}\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,  # Small limit to avoid extra words\n",
    "            do_sample=False  # Greedy decoding for deterministic output\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Example usage\n",
    "question = \"Janet's half-day egg is 16 eggs. He eats three in the morning and uses four daily for his friends to make muffins. The remaining eggs are sold at $ 2 per fresh egg in the farmer's market every day. How much money does he make every day at the farmer's market?\"\n",
    "numeric_answer = get_numeric_answer(question)\n",
    "print(\"Extracted Answer:\", numeric_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db8e1a6007047979c3a6cb39be684b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 19\u001b[0m\n\u001b[1;32m     11\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     12\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,                    \u001b[38;5;66;03m# Enable 4-bit quantization\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,            \u001b[38;5;66;03m# Options: 'nf4' (recommended) or 'fp4'\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,       \u001b[38;5;66;03m# Nested quantization for higher accuracy\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m     \u001b[38;5;66;03m# Mixed-precision type: 'fp16', 'bf16', etc.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 3. Load model in 4-bit\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# auto-distributes across GPU(s) if available\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4319\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4312\u001b[0m     (\n\u001b[1;32m   4313\u001b[0m         model,\n\u001b[1;32m   4314\u001b[0m         missing_keys,\n\u001b[1;32m   4315\u001b[0m         unexpected_keys,\n\u001b[1;32m   4316\u001b[0m         mismatched_keys,\n\u001b[1;32m   4317\u001b[0m         offload_index,\n\u001b[1;32m   4318\u001b[0m         error_msgs,\n\u001b[0;32m-> 4319\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4326\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4330\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4331\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4339\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4340\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4897\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4895\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4896\u001b[0m         fixed_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4897\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfixed_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4904\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4905\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4906\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4907\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4908\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4913\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4915\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:825\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# We convert floating dtypes to the `dtype` passed except for float8_e4m3fn type. We also want to keep the buffers/params\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# in int/uint/bool and not cast them.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m is_param_float8_e4m3fn \u001b[38;5;241m=\u001b[39m is_torch_e4m3fn_available \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat8_e4m3fn\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_param_float8_e4m3fn:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    827\u001b[0m         keep_in_fp32_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m    832\u001b[0m     ):\n\u001b[1;32m    833\u001b[0m         param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to the Hugging Face Hub\n",
    "login(token=\"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\")\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# 1. Create a quantization config for 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Options: 'nf4' (recommended) or 'fp4'\n",
    "    bnb_4bit_use_double_quant=True,       # Nested quantization for higher accuracy\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"     # Mixed-precision type: 'fp16', 'bf16', etc.\n",
    ")\n",
    "\n",
    "# 3. Load model in 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # auto-distributes across GPU(s) if available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BloomTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "# 2. Load tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Billyyy/llama_8K_extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "merged_dataset = load_dataset(\"Billyyy/llama_8K_extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_sequences_with_text(dataset, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Merge sequences in a dataset while keeping 'text' and 'input_ids' aligned.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset with 'text' and 'input_ids'.\n",
    "        tokenizer: Tokenizer used to tokenize the text.\n",
    "        max_length (int): Maximum length for merged sequences.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with merged 'text' and 'input_ids'.\n",
    "    \"\"\"\n",
    "    merged_texts = []\n",
    "    merged_input_ids = []\n",
    "    current_text = []\n",
    "    current_input_ids = []\n",
    "\n",
    "    for text, input_ids in zip(dataset[\"text\"], dataset[\"input_ids\"]):\n",
    "        # Check if adding the current sequence exceeds the max length\n",
    "        if len(current_input_ids) + len(input_ids) > max_length:\n",
    "            # Append the merged sequences\n",
    "            merged_texts.append(\" \".join(current_text))\n",
    "            merged_input_ids.append(current_input_ids[:max_length])\n",
    "            # Reset for the next sequence\n",
    "            current_text = []\n",
    "            current_input_ids = []\n",
    "        \n",
    "        # Extend the current sequence\n",
    "        current_text.append(text)\n",
    "        current_input_ids.extend(input_ids)\n",
    "    \n",
    "    # Add the final batch if it exists\n",
    "    if current_input_ids:\n",
    "        merged_texts.append(\" \".join(current_text))\n",
    "        merged_input_ids.append(current_input_ids[:max_length])\n",
    "\n",
    "    return {\"text\": merged_texts, \"input_ids\": merged_input_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=27): 100%|██████████| 955519/955519 [00:13<00:00, 72378.89 examples/s]\n",
      "Map (num_proc=27): 100%|██████████| 50291/50291 [00:00<00:00, 51975.37 examples/s]\n",
      "Map (num_proc=27): 100%|██████████| 955519/955519 [00:04<00:00, 221512.98 examples/s]\n",
      "Map (num_proc=27): 100%|██████████| 50291/50291 [00:00<00:00, 96506.03 examples/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.remove_columns(\"attention_mask\")\n",
    "\n",
    "# Add 'input_ids' column if not already in the dataset\n",
    "if \"input_ids\" not in dataset.column_names:\n",
    "    dataset = dataset.map(\n",
    "        lambda examples: {\"input_ids\": tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)[\"input_ids\"]},\n",
    "        batched=True,\n",
    "        num_proc=27\n",
    "    )\n",
    "\n",
    "# Apply the merging function\n",
    "merged_dataset = dataset.map(\n",
    "    lambda batch: merge_sequences_with_text(batch, tokenizer, max_length=256),\n",
    "    batched=True, \n",
    "    num_proc=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 645131\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 34004\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.remove_columns(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2325c3703bd3430fa15551b76269501f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdba4aeb0ad240db81f6dd6ce218a433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/216 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71d8d27663744e4ac618371af00bab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/216 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd79a7c4c3d4ec7954b293b8da3afbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/216 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee9f07a04b2490c87f99f5284a50b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376cf858e2f84a87b8edc869eff69530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/35 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c47bccca04a423fb81c0abc75a4b336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/423 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Billyyy/llama_8K_extended/commit/298dc49a0351e7853c5357e4cc5971b82ce2eed8', commit_message='Upload dataset', commit_description='', oid='298dc49a0351e7853c5357e4cc5971b82ce2eed8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Billyyy/llama_8K_extended', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Billyyy/llama_8K_extended'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.push_to_hub(\"Billyyy/llama_8K_extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenizes each line in the dataset.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=True, max_length=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090e826f234d46bca2a5b5a07c389784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=27):   0%|          | 0/645131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a46d69d8065493dbf0317f7688e1cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=27):   0%|          | 0/34004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_dataset = merged_dataset.map(tokenize_function, batched=True, num_proc=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mn> Хамгийн анх \"Халуун сэтгэл\" киноны найруулагч Л.Энхбаяр ах 2006 онд намайг кинонд уриад, гол дүрээ өгөхөд нь маш их баярлаж байлаа. <en>The first director of \"Hot Soul\", L.Enkhbayar, was excited to thank me for the role of the film in 2006. <mn> Манай хойд хєршийг хувьд ч гэсэн сурталчилгааны зорилгоор хуурамч мєнгийг ашиглах туршлагыг хэрэгжvvлж байжээ. <en>In addition, our northern neighbors have been involved in the use of counterfeit money for advertising.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset['train'][60000]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training Arguments -------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/ephemeral/llama_3B_translation\",  \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_dir=\"/workspace/logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    group_by_length=False,\n",
    "    \n",
    "    # A100-specific optimizations\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "\n",
    ")\n",
    "\n",
    "# 6. Data Collator for Masked Language Modeling -------------------\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# 7. Trainer Initialization ---------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=merged_dataset[\"train\"],\n",
    "    eval_dataset=merged_dataset[\"eval\"], \n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 645131\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc851ba7bd24b8aa0061906f620f863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 2. Load Pretrained Tokenizer & Model -----------------------------\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"  # Change if using another model\n",
    "TOKENIZER_PATH = \"Billyyy/llama_8K_extended\" # Path to your trained tokenizer\n",
    "DATASET_PATH = \"/workspace/labeled_corpus.txt\" \n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "# 3. Resize Model's Embedding Layer -------------------------------\n",
    "old_vocab_size = model.get_input_embeddings().num_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing model embeddings from 128256 → 135126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if vocab_size > old_vocab_size:\n",
    "    print(f\"Resizing model embeddings from {old_vocab_size} → {vocab_size}\")\n",
    "    model.resize_token_embeddings(vocab_size)\n",
    "\n",
    "# Freeze all layers except embedding\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze entire model\n",
    "\n",
    "# Enable training for embedding layer only\n",
    "model.get_input_embeddings().weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8,086,540,288\n",
      "Trainable parameters: 553,476,096\n",
      "Percentage of trainable parameters: 6.84%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    \"\"\"Returns the number of trainable parameters in the model.\"\"\"\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    return trainable_params\n",
    "\n",
    "# Example Usage (after loading your model)\n",
    "trainable_params = count_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 777, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 739, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/workspace/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/workspace/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 45, in __call__\n    return self.torch_call(features)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 943, in torch_call\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3397, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 241, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 793, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer.py:2500\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2498\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2499\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2500\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2502\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer.py:5180\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5179\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5180\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5181\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5182\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/accelerate/data_loader.py:564\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 564\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 777, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 739, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/workspace/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/workspace/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 45, in __call__\n    return self.torch_call(features)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 943, in torch_call\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3397, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 241, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/workspace/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 793, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = merged_dataset.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 **Sample 1:**\n",
      "Input IDs: [128000, 14066, 77, 29, 81529, 129063, 220, 128972, 129025, 130337, 220, 128594, 129051, 220, 128665, 134437, 130829, 129613, 220, 128989, 129461, 135069, 220, 129807, 131538, 220, 131143, 133635, 132778, 131651, 220, 131061, 128480, 129051, 134429, 128530, 129461, 129126, 220, 128813, 131160, 220, 128480, 133572, 130829, 131208, 131636, 128480, 129461, 131814, 129113, 45458, 130226, 128594, 130829, 130226, 129461, 220, 132551, 128989, 129461, 133545, 130829, 131143, 134429, 131061, 134429, 220, 128480, 131613, 128594, 129051, 220, 128697, 133208, 128594, 220, 130337, 134600, 129325, 220, 130829, 131061, 134429, 131763, 220, 134801, 128594, 130829, 133635, 129461, 133635, 129461, 130337, 220, 134899, 128480, 128382, 129461, 129461, 134076, 134429, 129561, 220, 134429, 128812, 128594, 220, 128972, 129025, 130337, 13, 366, 268, 43093, 3786, 374, 7373, 18641, 449, 44539, 89108, 6444, 889, 527, 1101, 39075, 18250, 1534, 449, 264, 11336, 3786, 13, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text: <mn> Энэ карт нь санхүүгийн илүү эрх чөлөөтэй амьдралын хэв маягийгэрхэмлэдэг Монгол харилцагчдад маань бүрэн тохирохоос гадна хөнгөлөлт урамшууллуудаар дүүрэн карт. <en>This card is fully compatible with Mongolian customers who are also financially liberalized with a discount card.\n",
      "Labels: [128000, 14066, 77, 29, 81529, 129063, 220, 128972, 129025, 130337, 220, 128594, 129051, 220, 128665, 134437, 130829, 129613, 220, 128989, 129461, 135069, 220, 129807, 131538, 220, 131143, 133635, 132778, 131651, 220, 131061, 128480, 129051, 134429, 128530, 129461, 129126, 220, 128813, 131160, 220, 128480, 133572, 130829, 131208, 131636, 128480, 129461, 131814, 129113, 45458, 130226, 128594, 130829, 130226, 129461, 220, 132551, 128989, 129461, 133545, 130829, 131143, 134429, 131061, 134429, 220, 128480, 131613, 128594, 129051, 220, 128697, 133208, 128594, 220, 130337, 134600, 129325, 220, 130829, 131061, 134429, 131763, 220, 134801, 128594, 130829, 133635, 129461, 133635, 129461, 130337, 220, 134899, 128480, 128382, 129461, 129461, 134076, 134429, 129561, 220, 134429, 128812, 128594, 220, 128972, 129025, 130337, 13, 366, 268, 43093, 3786, 374, 7373, 18641, 449, 44539, 89108, 6444, 889, 527, 1101, 39075, 18250, 1534, 449, 264, 11336, 3786, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "📝 **Sample 2:**\n",
      "Input IDs: [128000, 14066, 77, 29, 57855, 131160, 130337, 130226, 128697, 132556, 220, 134632, 128480, 130981, 128594, 129613, 220, 131061, 129082, 128989, 129461, 130337, 128935, 220, 131061, 129082, 128989, 129461, 220, 131923, 130033, 134935, 130050, 220, 133635, 128480, 130067, 220, 128480, 128552, 220, 131061, 131160, 130337, 130226, 128697, 132556, 128594, 134331, 220, 129840, 129025, 131143, 128712, 220, 129232, 129063, 220, 2636, 220, 130337, 133635, 130829, 134735, 130829, 220, 128697, 134820, 128594, 220, 128697, 130226, 129461, 220, 133635, 130067, 133635, 134429, 129606, 220, 7007, 11, 220, 134115, 129477, 128480, 220, 130008, 129461, 220, 128557, 132789, 130829, 131061, 128480, 129126, 220, 130337, 132663, 133761, 220, 134445, 220, 130337, 133635, 130829, 134735, 130829, 220, 128697, 130226, 129461, 130829, 130226, 129082, 220, 132961, 130829, 133019, 128594, 220, 128697, 133214, 131763, 13, 366, 268, 16761, 5951, 706, 7319, 279, 3430, 315, 220, 7007, 323, 1063, 3117, 8614, 44995, 311, 220, 134445, 386, 6542, 824, 5951, 1603, 279, 2883, 596, 8420, 1051, 51249, 369, 264, 5951, 8577, 13, 366, 22524, 29, 117482, 106, 129063, 128594, 131208, 220, 128813, 129461, 134454, 134429, 11, 220, 128697, 128989, 220, 134115, 131763, 129461, 130337, 220, 134429, 133214, 130050, 131763, 131061, 220, 131143, 220, 135125, 128438, 220, 131061, 129461, 134429, 131613, 130829, 220, 134429, 131061, 131160, 130337, 132275, 133214, 220, 130829, 131814, 129082, 220, 131538, 132549, 130041, 13, 366, 268, 66430, 8206, 11, 358, 656, 539, 1390, 311, 13454, 856, 16930, 13, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text: <mn> Автобус компанийн ажилтнууд ажил хаяхаас өмнө микро автобусны зорчих үнэ 500 төгрөг байсан бол өнөөдөр 700, зарим хол шугамын тарифыг 1000 төгрөг болгож өсгөсөн байна. <en>The bus has increased the price of 700 and some far-line tariffs to 1000 MNT per bus before the company's employees were tossed for a bus trip. <mn> Үнэнийг хэлэхэд, би заналт дайснаа ч миний алдааг давтаасай гэж хүсэхгүй. <en>In truth, I do not want to repeat my mistake.\n",
      "Labels: [128000, 14066, 77, 29, 57855, 131160, 130337, 130226, 128697, 132556, 220, 134632, 128480, 130981, 128594, 129613, 220, 131061, 129082, 128989, 129461, 130337, 128935, 220, 131061, 129082, 128989, 129461, 220, 131923, 130033, 134935, 130050, 220, 133635, 128480, 130067, 220, 128480, 128552, 220, 131061, 131160, 130337, 130226, 128697, 132556, 128594, 134331, 220, 129840, 129025, 131143, 128712, 220, 129232, 129063, 220, 2636, 220, 130337, 133635, 130829, 134735, 130829, 220, 128697, 134820, 128594, 220, 128697, 130226, 129461, 220, 133635, 130067, 133635, 134429, 129606, 220, 7007, 11, 220, 134115, 129477, 128480, 220, 130008, 129461, 220, 128557, 132789, 130829, 131061, 128480, 129126, 220, 130337, 132663, 133761, 220, 134445, 220, 130337, 133635, 130829, 134735, 130829, 220, 128697, 130226, 129461, 130829, 130226, 129082, 220, 132961, 130829, 133019, 128594, 220, 128697, 133214, 131763, 13, 366, 268, 16761, 5951, 706, 7319, 279, 3430, 315, 220, 7007, 323, 1063, 3117, 8614, 44995, 311, 220, 134445, 386, 6542, 824, 5951, 1603, 279, 2883, 596, 8420, 1051, 51249, 369, 264, 5951, 8577, 13, 366, 22524, 29, 117482, 106, 129063, 128594, 131208, 220, 128813, 129461, 134454, 134429, 11, 220, 128697, 128989, 220, 134115, 131763, 129461, 130337, 220, 134429, 133214, 130050, 131763, 131061, 220, 131143, 220, 135125, 128438, 220, 131061, 129461, 134429, 131613, 130829, 220, 134429, 131061, 131160, 130337, 132275, 133214, 220, 130829, 131814, 129082, 220, 131538, 132549, 130041, 13, 366, 268, 66430, 8206, 11, 358, 656, 539, 1390, 311, 13454, 856, 16930, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "📝 **Sample 3:**\n",
      "Input IDs: [128000, 14066, 77, 29, 57855, 131318, 131061, 128594, 130829, 133214, 220, 133214, 128480, 131061, 130829, 130337, 220, 130050, 135069, 129461, 129613, 220, 130829, 132068, 131160, 131061, 128594, 220, 128756, 130829, 134076, 129461, 129051, 220, 134429, 128735, 129461, 131061, 128594, 59842, 31274, 102357, 12, 134429, 57855, 129025, 134429, 131143, 128989, 129461, 128665, 220, 131763, 128480, 129126, 220, 130829, 132401, 134429, 131208, 220, 128756, 130829, 131223, 128594, 220, 129461, 220, 134429, 129968, 13, 366, 268, 66430, 279, 22791, 21313, 2192, 9395, 351, 11, 279, 1566, 2380, 16374, 1051, 16689, 311, 279, 20302, 304, 279, 20302, 13, 366, 22524, 29, 35448, 134429, 129968, 43896, 131143, 128989, 130194, 129082, 131961, 130829, 220, 134429, 131814, 128480, 129082, 128989, 129082, 11, 220, 131814, 129461, 134429, 131814, 131160, 220, 128480, 134916, 220, 134115, 131061, 130829, 131483, 220, 133635, 130829, 131143, 128712, 220, 131160, 128438, 220, 128813, 128480, 132751, 128594, 220, 128712, 131814, 134429, 220, 133214, 129082, 220, 128697, 133214, 130829, 131613, 220, 131061, 129082, 13, 366, 268, 29, 7184, 11, 26946, 343, 53478, 374, 1633, 27207, 430, 568, 690, 539, 3041, 709, 13, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text: <mn> Архангай аймагт сүүлийн гурван сонгууль дараалан УИХ-д Ардчилсан намын гишүүдийг сонгосон л доо. <en>In the Arkhangai aimag, the last three elections were elected to the Parliament in the Parliament. <mn> Одоо Ичиножёог дэмжиж, элдэв мэх заагаад өгчих вий хэмээн ихэд айж байгаа аж. <en>Now, Ichigoyo is very scared that he will not give up.\n",
      "Labels: [128000, 14066, 77, 29, 57855, 131318, 131061, 128594, 130829, 133214, 220, 133214, 128480, 131061, 130829, 130337, 220, 130050, 135069, 129461, 129613, 220, 130829, 132068, 131160, 131061, 128594, 220, 128756, 130829, 134076, 129461, 129051, 220, 134429, 128735, 129461, 131061, 128594, 59842, 31274, 102357, 12, 134429, 57855, 129025, 134429, 131143, 128989, 129461, 128665, 220, 131763, 128480, 129126, 220, 130829, 132401, 134429, 131208, 220, 128756, 130829, 131223, 128594, 220, 129461, 220, 134429, 129968, 13, 366, 268, 66430, 279, 22791, 21313, 2192, 9395, 351, 11, 279, 1566, 2380, 16374, 1051, 16689, 311, 279, 20302, 304, 279, 20302, 13, 366, 22524, 29, 35448, 134429, 129968, 43896, 131143, 128989, 130194, 129082, 131961, 130829, 220, 134429, 131814, 128480, 129082, 128989, 129082, 11, 220, 131814, 129461, 134429, 131814, 131160, 220, 128480, 134916, 220, 134115, 131061, 130829, 131483, 220, 133635, 130829, 131143, 128712, 220, 131160, 128438, 220, 128813, 128480, 132751, 128594, 220, 128712, 131814, 134429, 220, 133214, 129082, 220, 128697, 133214, 130829, 131613, 220, 131061, 129082, 13, 366, 268, 29, 7184, 11, 26946, 343, 53478, 374, 1633, 27207, 430, 568, 690, 539, 3041, 709, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "📝 **Sample 4:**\n",
      "Input IDs: [128000, 14066, 77, 29, 100980, 128480, 129127, 128585, 220, 131143, 220, 130379, 131143, 128989, 128594, 348, 132580, 129613, 220, 128697, 132504, 128545, 220, 128697, 133214, 130829, 134076, 129461, 131061, 128480, 129082, 220, 128594, 129051, 220, 131814, 129063, 220, 130008, 130337, 129126, 220, 131814, 128594, 130829, 129807, 220, 134429, 130043, 220, 130337, 132789, 128594, 220, 130610, 130033, 131538, 131763, 135117, 220, 128697, 129022, 131143, 220, 134454, 129461, 129985, 220, 131061, 129082, 13, 366, 268, 29, 11458, 11, 6617, 14016, 6137, 311, 10205, 1633, 6051, 304, 279, 3363, 13, 366, 22524, 29, 51418, 132935, 129025, 220, 129082, 128989, 128594, 128813, 129063, 220, 130226, 129461, 130226, 128594, 220, 131538, 132789, 131160, 128989, 129461, 133545, 128530, 129127, 220, 130337, 130226, 130829, 129461, 130226, 130577, 220, 131143, 220, 135102, 128594, 128438, 220, 131923, 128480, 130829, 131613, 129461, 131061, 130829, 131143, 129613, 220, 130637, 130829, 132751, 220, 131763, 128884, 134429, 131160, 129127, 128585, 220, 128697, 128895, 129461, 135069, 129461, 129113, 220, 129063, 130829, 131814, 128594, 13, 366, 268, 29, 1548, 11335, 304, 1690, 3953, 11, 719, 568, 374, 279, 832, 889, 27772, 813, 477, 1077, 29315, 596, 3560, 21676, 13, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257, 128257]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text: <mn> Ямартай ч орчин vеийн барилга байгууламж нь энэ хотын энгэр дээр тун саяхнаас босч эхэлсэн аж. <en>However, modern buildings began to rise very recently in the city. <mn> Тэрээр жинхэнэ олон хувилцараар тоглодог ч зүүний хамгаалагчийн үүргээ найдвартай биелүүлдэг нэгэн. <en>He plays in many games, but he is the one who performs his or her defender's role safely.\n",
      "Labels: [128000, 14066, 77, 29, 100980, 128480, 129127, 128585, 220, 131143, 220, 130379, 131143, 128989, 128594, 348, 132580, 129613, 220, 128697, 132504, 128545, 220, 128697, 133214, 130829, 134076, 129461, 131061, 128480, 129082, 220, 128594, 129051, 220, 131814, 129063, 220, 130008, 130337, 129126, 220, 131814, 128594, 130829, 129807, 220, 134429, 130043, 220, 130337, 132789, 128594, 220, 130610, 130033, 131538, 131763, 135117, 220, 128697, 129022, 131143, 220, 134454, 129461, 129985, 220, 131061, 129082, 13, 366, 268, 29, 11458, 11, 6617, 14016, 6137, 311, 10205, 1633, 6051, 304, 279, 3363, 13, 366, 22524, 29, 51418, 132935, 129025, 220, 129082, 128989, 128594, 128813, 129063, 220, 130226, 129461, 130226, 128594, 220, 131538, 132789, 131160, 128989, 129461, 133545, 128530, 129127, 220, 130337, 130226, 130829, 129461, 130226, 130577, 220, 131143, 220, 135102, 128594, 128438, 220, 131923, 128480, 130829, 131613, 129461, 131061, 130829, 131143, 129613, 220, 130637, 130829, 132751, 220, 131763, 128884, 134429, 131160, 129127, 128585, 220, 128697, 128895, 129461, 135069, 129461, 129113, 220, 129063, 130829, 131814, 128594, 13, 366, 268, 29, 1548, 11335, 304, 1690, 3953, 11, 719, 568, 374, 279, 832, 889, 27772, 813, 477, 1077, 29315, 596, 3560, 21676, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def show_collated_batch(dataset, tokenizer, data_collator, batch_size=4):\n",
    "    \"\"\"Fetches and prints a sample batch after applying the DataCollator.\"\"\"\n",
    "    \n",
    "    # Create DataLoader with DataCollator\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "    \n",
    "    # Get one batch\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        print(f\"\\n📝 **Sample {i+1}:**\")\n",
    "        print(\"Input IDs:\", batch[\"input_ids\"][i].tolist())\n",
    "        print(\"Attention Mask:\", batch[\"attention_mask\"][i].tolist())\n",
    "\n",
    "        # Decode tokenized input back to text\n",
    "        decoded_text = tokenizer.decode(batch[\"input_ids\"][i], skip_special_tokens=True)\n",
    "        print(\"Decoded Text:\", decoded_text)\n",
    "\n",
    "        if \"labels\" in batch:\n",
    "            print(\"Labels:\", batch[\"labels\"][i].tolist())\n",
    "\n",
    "# Initialize Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Example Usage\n",
    "show_collated_batch(merged_dataset['train'], tokenizer, data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model detected: Using `self.model.model.decoder.layers`\n",
      "LoRA model detected: Using `self.model.base_model.model.decoder.layers`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "class LogitLensAnalyzer:\n",
    "    def __init__(self, peft_model=None, device=\"cpu\", top_k=5, num_tokens=5):\n",
    "        \"\"\"\n",
    "        Initializes the Logit Lens analysis for NLLB models.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.top_k = top_k  # Number of top tokens to extract per layer\n",
    "        self.num_tokens = num_tokens  # Number of tokens to analyze\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\", src_lang=\"khk_Cyrl\")\n",
    "        self.hidden_states_per_step = []  # Store hidden states at each step\n",
    "        self.peft_model = peft_model\n",
    "\n",
    "        if self.peft_model:\n",
    "            # Load the base model and LoRA-adapted model\n",
    "            base_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-1.3B\").to(device)\n",
    "            self.model = PeftModel.from_pretrained(base_model, peft_model).to(device)\n",
    "        else:\n",
    "            # Load standard NLLB model\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-1.3B\").to(device)\n",
    "\n",
    "        # Automatically detect where the decoder layers are located\n",
    "        self.decoder_layers = self._find_decoder_layers()\n",
    "        self.lm_head = self.model.lm_head  # The final layer that maps hidden states to logits\n",
    "\n",
    "    def _find_decoder_layers(self):\n",
    "        \"\"\"Finds the correct path to the decoder layers dynamically.\"\"\"\n",
    "        if self.peft_model:\n",
    "            print(\"LoRA model detected: Using `self.model.base_model.model.decoder.layers`\")\n",
    "            return self.model.model.model.decoder.layers  # LoRA-adapted model\n",
    "        else:\n",
    "            print(\"Base model detected: Using `self.model.model.decoder.layers`\")\n",
    "            return self.model.model.decoder.layers  # Standard model\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        \"\"\"Captures hidden states from all decoder layers.\"\"\"\n",
    "        self.hidden_states_per_step[-1].append(output[0])  # Extract the main hidden state\n",
    "\n",
    "    def register_hooks(self):\n",
    "        \"\"\"Registers hooks for ALL decoder layers.\"\"\"\n",
    "        self.hooks = []\n",
    "        for layer in self.decoder_layers:\n",
    "            hook = layer.register_forward_hook(self.hook_fn)\n",
    "            self.hooks.append(hook)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Removes hooks after execution.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def extract_logits_autoregressive(self, input_text):\n",
    "        \"\"\"\n",
    "        Runs input text through the model and extracts hidden states while generating tokens autoregressively.\n",
    "        \"\"\"\n",
    "        self.hidden_states_per_step = []\n",
    "        self.register_hooks()\n",
    "\n",
    "        # Tokenize the input and move to device\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Prepare encoder outputs (do this once to avoid redundant computation)\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.model.get_encoder()(**inputs, return_dict=True)\n",
    "\n",
    "        # Initialize decoder with the correct start token\n",
    "        if self.model.config.decoder_start_token_id is not None:\n",
    "            decoder_input_ids = torch.tensor([[self.model.config.decoder_start_token_id]], device=self.device)\n",
    "        else:\n",
    "            decoder_input_ids = torch.tensor([[self.tokenizer.bos_token_id]], device=self.device)  # Fallback\n",
    "\n",
    "        past_key_values = None  # Store cached past key values for efficient decoding\n",
    "        generated_tokens = []\n",
    "\n",
    "        print(f\"\\n🔹 **Starting autoregressive generation for input:** '{input_text}'\\n\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step in range(self.num_tokens):\n",
    "                self.hidden_states_per_step.append([])  # Prepare storage for this step\n",
    "\n",
    "                # Forward pass with past key values\n",
    "                outputs = self.model(\n",
    "                    encoder_outputs=encoder_outputs,\n",
    "                    decoder_input_ids=decoder_input_ids,\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,  # Enables faster decoding\n",
    "                    output_hidden_states=True,  # Ensures hidden states are returned\n",
    "                    return_dict=True\n",
    "                )\n",
    "\n",
    "                logits = outputs.logits[:, -1, :]  # Logits for the last generated token\n",
    "                past_key_values = outputs.past_key_values  # Update cache for next step\n",
    "\n",
    "                # Store hidden states for this step\n",
    "                self.hidden_states_per_step[-1] = outputs.decoder_hidden_states  # Capture hidden states for this step\n",
    "\n",
    "                # Select next token (greedy decoding)\n",
    "                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                generated_tokens.append(next_token.item())\n",
    "\n",
    "                # Convert token ID to text and print debug info\n",
    "                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "                print(f\"🔹 **Step {step+1}: Generated Token ID:** {next_token.item()} -> '{self.tokenizer.decode(next_token.item())}'\")\n",
    "                print(f\"   **Current Generated Text:** '{generated_text}'\\n\")\n",
    "\n",
    "                # Append new token to decoder input for next step\n",
    "                decoder_input_ids = next_token  # Keep only the latest token\n",
    "\n",
    "                # Stop if EOS token is generated\n",
    "                if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                    print(\"🔹 **EOS token detected. Stopping generation.**\")\n",
    "                    break\n",
    "\n",
    "        self.remove_hooks()\n",
    "        return self.hidden_states_per_step, generated_tokens  # Return all recorded hidden states and generated tokens\n",
    "\n",
    "\n",
    "\n",
    "    def get_top_k_predictions(self, hidden_states):\n",
    "        \"\"\"Extracts top-k predictions for each layer at each step.\"\"\"\n",
    "        top_k_predictions = []\n",
    "\n",
    "        for layer_states in hidden_states:  # Iterate over decoder layers\n",
    "            logits = self.lm_head(layer_states[:, -1, :])  # Compute logits\n",
    "            top_k_values, top_k_indices = torch.topk(logits, self.top_k, dim=-1)\n",
    "            decoded_tokens = [self.tokenizer.decode(idx.item()) for idx in top_k_indices[0]]\n",
    "            top_k_predictions.append(decoded_tokens)\n",
    "\n",
    "        return top_k_predictions  # List of [layer][top-k words]\n",
    "\n",
    "    def compare_models(self, base_hidden_states, lora_hidden_states):\n",
    "        \"\"\"\n",
    "        Compares hidden states from base and LoRA models and extracts logit predictions.\n",
    "        \"\"\"\n",
    "        min_steps = min(len(base_hidden_states), len(lora_hidden_states))  # Ensure matching lengths\n",
    "        similarities = []\n",
    "        entropies = []\n",
    "        base_top_k_tokens = []\n",
    "        lora_top_k_tokens = []\n",
    "\n",
    "        for step in range(min_steps):  # Step-wise comparison\n",
    "            base_step_states = base_hidden_states[step]\n",
    "            lora_step_states = lora_hidden_states[step]\n",
    "\n",
    "            step_similarities = []\n",
    "            step_entropies = []\n",
    "\n",
    "            # Compute top-k token predictions\n",
    "            base_top_k_tokens.append(self.get_top_k_predictions(base_step_states))\n",
    "            lora_top_k_tokens.append(self.get_top_k_predictions(lora_step_states))\n",
    "\n",
    "            for base_h, lora_h in zip(base_step_states, lora_step_states):\n",
    "                # Compute cosine similarity\n",
    "                cos_sim = F.cosine_similarity(base_h, lora_h, dim=-1).mean().item()\n",
    "                step_similarities.append(cos_sim)\n",
    "\n",
    "                # Compute entropy\n",
    "                probs = torch.nn.functional.softmax(lora_h, dim=-1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1).mean().item()\n",
    "                step_entropies.append(entropy)\n",
    "\n",
    "            similarities.append(step_similarities)\n",
    "            entropies.append(step_entropies)\n",
    "\n",
    "        return similarities, entropies, base_top_k_tokens, lora_top_k_tokens\n",
    "\n",
    "# Load models\n",
    "base_model_analyzer = LogitLensAnalyzer(num_tokens=5)  # Analyze 5 tokens\n",
    "lora_model_analyzer = LogitLensAnalyzer(peft_model=\"Billyyy/mon_nllb_3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 **Starting autoregressive generation for input:** 'британийн фунт'\n",
      "\n",
      "🔹 **Step 1: Generated Token ID:** 256047 -> 'eng_Latn'\n",
      "   **Current Generated Text:** ''\n",
      "\n",
      "🔹 **Step 2: Generated Token ID:** 43504 -> 'British'\n",
      "   **Current Generated Text:** 'British'\n",
      "\n",
      "🔹 **Step 3: Generated Token ID:** 218790 -> 'pound'\n",
      "   **Current Generated Text:** 'British pound'\n",
      "\n",
      "🔹 **Step 4: Generated Token ID:** 2 -> '</s>'\n",
      "   **Current Generated Text:** 'British pound'\n",
      "\n",
      "🔹 **EOS token detected. Stopping generation.**\n",
      "\n",
      "🔹 **Starting autoregressive generation for input:** 'британийн фунт'\n",
      "\n",
      "🔹 **Step 1: Generated Token ID:** 256047 -> 'eng_Latn'\n",
      "   **Current Generated Text:** ''\n",
      "\n",
      "🔹 **Step 2: Generated Token ID:** 43504 -> 'British'\n",
      "   **Current Generated Text:** 'British'\n",
      "\n",
      "🔹 **Step 3: Generated Token ID:** 1920 -> 'Po'\n",
      "   **Current Generated Text:** 'British Po'\n",
      "\n",
      "🔹 **Step 4: Generated Token ID:** 514 -> 'und'\n",
      "   **Current Generated Text:** 'British Pound'\n",
      "\n",
      "🔹 **Step 5: Generated Token ID:** 248075 -> '.'\n",
      "   **Current Generated Text:** 'British Pound.'\n",
      "\n",
      "\n",
      "🔹 **Comparison of Base and LoRA Model Logit Predictions** 🔹\n",
      "\n",
      "🟢 Step 1:\n",
      "   - 🔹 Layer 1 Base: ['</s>', '.', '.', '(', '-']\n",
      "   - 🔸 Layer 1 LoRA: ['</s>', '.', '.', '(', '-']\n",
      "   - 🔹 Layer 2 Base: ['eng_Latn', 'fra_Latn', 'ibo_Latn', 'sin_Sinh', 'sna_Latn']\n",
      "   - 🔸 Layer 2 LoRA: ['eng_Latn', 'fra_Latn', 'ibo_Latn', 'sin_Sinh', 'sot_Latn']\n",
      "   - 🔹 Layer 3 Base: ['eng_Latn', 'fra_Latn', 'ibo_Latn', 'sin_Sinh', 'sna_Latn']\n",
      "   - 🔸 Layer 3 LoRA: ['eng_Latn', 'fra_Latn', 'ibo_Latn', 'sin_Sinh', 'sna_Latn']\n",
      "   - 🔹 Layer 4 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'sna_Latn']\n",
      "   - 🔸 Layer 4 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'sna_Latn']\n",
      "   - 🔹 Layer 5 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'sna_Latn']\n",
      "   - 🔸 Layer 5 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'sna_Latn']\n",
      "   - 🔹 Layer 6 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔸 Layer 6 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'sna_Latn']\n",
      "   - 🔹 Layer 7 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔸 Layer 7 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔹 Layer 8 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔸 Layer 8 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔹 Layer 9 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔸 Layer 9 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔹 Layer 10 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔸 Layer 10 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔹 Layer 11 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔸 Layer 11 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'tam_Taml']\n",
      "   - 🔹 Layer 12 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔸 Layer 12 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'tam_Taml']\n",
      "   - 🔹 Layer 13 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'mal_Mlym', 'ibo_Latn']\n",
      "   - 🔸 Layer 13 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'tam_Taml']\n",
      "   - 🔹 Layer 14 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'mal_Mlym', 'tam_Taml']\n",
      "   - 🔸 Layer 14 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'tam_Taml', 'mal_Mlym']\n",
      "   - 🔹 Layer 15 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'mal_Mlym', 'tam_Taml']\n",
      "   - 🔸 Layer 15 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'tam_Taml', 'mal_Mlym']\n",
      "   - 🔹 Layer 16 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'mal_Mlym', 'tam_Taml']\n",
      "   - 🔸 Layer 16 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'ibo_Latn', 'mal_Mlym']\n",
      "   - 🔹 Layer 17 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'mal_Mlym', 'tam_Taml']\n",
      "   - 🔸 Layer 17 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'sna_Latn', 'ibo_Latn']\n",
      "   - 🔹 Layer 18 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'mal_Mlym', 'tam_Taml']\n",
      "   - 🔸 Layer 18 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'tso_Latn', 'ibo_Latn']\n",
      "   - 🔹 Layer 19 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'mal_Mlym', 'tam_Taml']\n",
      "   - 🔸 Layer 19 LoRA: ['eng_Latn', 'fra_Latn', 'afr_Latn', 'swh_Latn', 'ibo_Latn']\n",
      "   - 🔹 Layer 20 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'mal_Mlym', 'epo_Latn']\n",
      "   - 🔸 Layer 20 LoRA: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'epo_Latn', 'afr_Latn']\n",
      "   - 🔹 Layer 21 Base: ['eng_Latn', 'fra_Latn', 'sin_Sinh', 'epo_Latn', 'mal_Mlym']\n",
      "   - 🔸 Layer 21 LoRA: ['eng_Latn', 'fra_Latn', 'epo_Latn', 'sin_Sinh', 'afr_Latn']\n",
      "   - 🔹 Layer 22 Base: ['eng_Latn', 'epo_Latn', 'fra_Latn', 'sin_Sinh', 'jpn_Jpan']\n",
      "   - 🔸 Layer 22 LoRA: ['eng_Latn', 'epo_Latn', 'fra_Latn', 'hin_Deva', 'eus_Latn']\n",
      "   - 🔹 Layer 23 Base: ['eng_Latn', 'epo_Latn', 'jpn_Jpan', 'eus_Latn', 'fra_Latn']\n",
      "   - 🔸 Layer 23 LoRA: ['eng_Latn', 'epo_Latn', 'eus_Latn', 'fra_Latn', 'afr_Latn']\n",
      "   - 🔹 Layer 24 Base: ['eng_Latn', 'epo_Latn', 'eus_Latn', 'jpn_Jpan', 'uig_Arab']\n",
      "   - 🔸 Layer 24 LoRA: ['eng_Latn', 'epo_Latn', 'eus_Latn', 'fra_Latn', 'afr_Latn']\n",
      "   - 🔹 Layer 25 Base: ['eng_Latn', 'epo_Latn', 'eus_Latn', 'jpn_Jpan', 'uig_Arab']\n",
      "   - 🔸 Layer 25 LoRA: ['eng_Latn', 'eus_Latn', 'epo_Latn', '</s>', 'swh_Latn']\n",
      "\n",
      "🟢 Step 2:\n",
      "   - 🔹 Layer 1 Base: ['eng_Latn', 'eus_Latn', 'epo_Latn', 'nno_Latn', 'Here']\n",
      "   - 🔸 Layer 1 LoRA: ['eng_Latn', 'eus_Latn', 'epo_Latn', 'nno_Latn', 'Here']\n",
      "   - 🔹 Layer 2 Base: ['eng_Latn', 'Here', 'This', 'When', 'It']\n",
      "   - 🔸 Layer 2 LoRA: ['eng_Latn', 'Here', 'This', 'Both', 'Now']\n",
      "   - 🔹 Layer 3 Base: ['eng_Latn', 'However', 'Here', 'This', 'When']\n",
      "   - 🔸 Layer 3 LoRA: ['eng_Latn', 'Here', 'This', 'When', 'Like']\n",
      "   - 🔹 Layer 4 Base: ['eng_Latn', 'This', 'However', 'Here', 'But']\n",
      "   - 🔸 Layer 4 LoRA: ['eng_Latn', 'Here', 'This', 'But', 'When']\n",
      "   - 🔹 Layer 5 Base: ['eng_Latn', 'However', 'This', 'But', 'Here']\n",
      "   - 🔸 Layer 5 LoRA: ['eng_Latn', 'Here', 'This', 'But', 'It']\n",
      "   - 🔹 Layer 6 Base: ['This', 'Then', 'eng_Latn', 'Here', 'And']\n",
      "   - 🔸 Layer 6 LoRA: ['Here', 'Then', 'This', 'Now', 'But']\n",
      "   - 🔹 Layer 7 Base: ['This', 'Then', 'Here', 'However', 'Also']\n",
      "   - 🔸 Layer 7 LoRA: ['Here', 'Now', 'Well', 'This', 'But']\n",
      "   - 🔹 Layer 8 Base: ['This', 'Then', 'The', 'And', 'Well']\n",
      "   - 🔸 Layer 8 LoRA: ['Well', 'Here', 'This', 'But', 'Hey']\n",
      "   - 🔹 Layer 9 Base: ['This', 'The', 'Then', 'And', 'It']\n",
      "   - 🔸 Layer 9 LoRA: ['Well', 'Hey', 'This', 'Now', 'But']\n",
      "   - 🔹 Layer 10 Base: ['The', 'This', 'Then', 'It', 'And']\n",
      "   - 🔸 Layer 10 LoRA: ['Well', 'Hey', 'Now', 'The', 'But']\n",
      "   - 🔹 Layer 11 Base: ['The', 'This', 'It', 'Then', 'And']\n",
      "   - 🔸 Layer 11 LoRA: ['Well', 'Hey', 'The', '-', 'Yeah']\n",
      "   - 🔹 Layer 12 Base: ['The', 'This', 'It', '-', 'Then']\n",
      "   - 🔸 Layer 12 LoRA: ['Well', '-', 'The', 'Hey', 'Yeah']\n",
      "   - 🔹 Layer 13 Base: ['The', 'This', 'It', '-', 'And']\n",
      "   - 🔸 Layer 13 LoRA: ['Well', 'The', '-', 'Yeah', 'Oh']\n",
      "   - 🔹 Layer 14 Base: ['The', 'This', 'It', '-', 'I']\n",
      "   - 🔸 Layer 14 LoRA: ['The', '-', 'Well', 'It', 'Oh']\n",
      "   - 🔹 Layer 15 Base: ['The', 'the', '-', 'I', '(']\n",
      "   - 🔸 Layer 15 LoRA: ['The', '-', 'Well', 'It', 'Oh']\n",
      "   - 🔹 Layer 16 Base: ['The', 'the', 'and', '-', '(']\n",
      "   - 🔸 Layer 16 LoRA: ['The', '-', 'Well', 'It', 'I']\n",
      "   - 🔹 Layer 17 Base: ['the', 'The', 'and', '(', 'I']\n",
      "   - 🔸 Layer 17 LoRA: ['The', '-', 'It', 'I', 'Well']\n",
      "   - 🔹 Layer 18 Base: ['the', 'The', 'and', '(', 'I']\n",
      "   - 🔸 Layer 18 LoRA: ['The', '-', 'I', 'the', 'It']\n",
      "   - 🔹 Layer 19 Base: ['the', 'The', 'and', '(', 'British']\n",
      "   - 🔸 Layer 19 LoRA: ['The', '-', 'British', '(', 'the']\n",
      "   - 🔹 Layer 20 Base: ['the', 'The', 'and', 'British', '(']\n",
      "   - 🔸 Layer 20 LoRA: ['The', 'British', '-', 'the', '(']\n",
      "   - 🔹 Layer 21 Base: ['the', 'The', 'British', 'and', '(']\n",
      "   - 🔸 Layer 21 LoRA: ['British', 'The', '-', 'the', '(']\n",
      "   - 🔹 Layer 22 Base: ['the', 'British', 'The', 'and', '(']\n",
      "   - 🔸 Layer 22 LoRA: ['British', 'The', '-', 'the', '(']\n",
      "   - 🔹 Layer 23 Base: ['the', 'British', 'The', '(', 'and']\n",
      "   - 🔸 Layer 23 LoRA: ['British', 'The', 'the', '-', '(']\n",
      "   - 🔹 Layer 24 Base: ['the', 'British', 'The', 'in', 'and']\n",
      "   - 🔸 Layer 24 LoRA: ['British', 'The', 'the', '(', '-']\n",
      "   - 🔹 Layer 25 Base: ['British', 'the', 'The', 'and', 'Po']\n",
      "   - 🔸 Layer 25 LoRA: ['British', 'The', '£', 'Great', 'Po']\n",
      "\n",
      "🟢 Step 3:\n",
      "   - 🔹 Layer 1 Base: ['British', 'brit', 'American', 'Britain', 'UK']\n",
      "   - 🔸 Layer 1 LoRA: ['British', 'brit', 'American', 'Britain', 'UK']\n",
      "   - 🔹 Layer 2 Base: ['British', 'American', 'brit', 'English', 'Britain']\n",
      "   - 🔸 Layer 2 LoRA: ['British', 'American', 'brit', 'Britain', 'English']\n",
      "   - 🔹 Layer 3 Base: ['British', 'American', 'English', 'Britain', 'brit']\n",
      "   - 🔸 Layer 3 LoRA: ['British', 'American', 'English', 'Britain', 'brit']\n",
      "   - 🔹 Layer 4 Base: ['British', 'American', 'English', 'brit', 'Britain']\n",
      "   - 🔸 Layer 4 LoRA: ['British', 'American', 'English', 'brit', 'Britain']\n",
      "   - 🔹 Layer 5 Base: ['British', 'American', 'English', 'brit', 'Britain']\n",
      "   - 🔸 Layer 5 LoRA: ['British', 'English', 'American', 'Britain', 'brit']\n",
      "   - 🔹 Layer 6 Base: ['British', 'American', 'English', 'Britain', 'French']\n",
      "   - 🔸 Layer 6 LoRA: ['British', 'English', 'American', 'Britain', 'French']\n",
      "   - 🔹 Layer 7 Base: ['British', 'English', 'American', 'French', 'Parliament']\n",
      "   - 🔸 Layer 7 LoRA: ['British', 'English', 'American', 'French', 'Britain']\n",
      "   - 🔹 Layer 8 Base: ['British', 'English', 'American', 'Parliament', 'Indian']\n",
      "   - 🔸 Layer 8 LoRA: ['British', 'English', 'American', 'Parliament', 'French']\n",
      "   - 🔹 Layer 9 Base: ['British', 'English', 'American', 'Parliament', 'Indian']\n",
      "   - 🔸 Layer 9 LoRA: ['British', 'English', 'American', 'Parliament', 'French']\n",
      "   - 🔹 Layer 10 Base: ['British', 'American', 'English', 'Parliament', 'military']\n",
      "   - 🔸 Layer 10 LoRA: ['British', 'American', 'English', 'Parliament', 'military']\n",
      "   - 🔹 Layer 11 Base: ['British', 'English', 'American', 'Empire', 'Parliament']\n",
      "   - 🔸 Layer 11 LoRA: ['British', 'English', 'Empire', 'American', 'military']\n",
      "   - 🔹 Layer 12 Base: ['British', 'English', 'military', 'Empire', 'American']\n",
      "   - 🔸 Layer 12 LoRA: ['British', 'Empire', 'military', 'English', 'Army']\n",
      "   - 🔹 Layer 13 Base: ['British', 'Empire', 'military', 'Parliament', 'English']\n",
      "   - 🔸 Layer 13 LoRA: ['British', 'Empire', 'military', 'Army', 'Parliament']\n",
      "   - 🔹 Layer 14 Base: ['British', 'Empire', 'military', 'currency', 'citizens']\n",
      "   - 🔸 Layer 14 LoRA: ['British', 'Empire', 'military', 'currency', 'Army']\n",
      "   - 🔹 Layer 15 Base: ['British', 'currency', 'Empire', 'dollar', 'military']\n",
      "   - 🔸 Layer 15 LoRA: ['Empire', 'British', 'dollar', 'currency', 'military']\n",
      "   - 🔹 Layer 16 Base: ['currency', 'dollar', 'British', 'pound', 'dollars']\n",
      "   - 🔸 Layer 16 LoRA: ['currency', 'dollar', 'dollars', 'British', 'money']\n",
      "   - 🔹 Layer 17 Base: ['pound', 'currency', 'dollar', 'pounds', 'British']\n",
      "   - 🔸 Layer 17 LoRA: ['dollar', 'pound', 'dollars', 'Dollar', 'currency']\n",
      "   - 🔹 Layer 18 Base: ['pound', 'dollar', 'pounds', 'currency', '£']\n",
      "   - 🔸 Layer 18 LoRA: ['pound', 'dollar', 'pounds', '£', 'money']\n",
      "   - 🔹 Layer 19 Base: ['pound', 'pounds', 'currency', 'dollar', '£']\n",
      "   - 🔸 Layer 19 LoRA: ['pound', 'pounds', '£', 'dollar', 'dollars']\n",
      "   - 🔹 Layer 20 Base: ['pound', 'pounds', '£', 'currency', 'dollar']\n",
      "   - 🔸 Layer 20 LoRA: ['pound', 'pounds', '£', 'Po', 'dollar']\n",
      "   - 🔹 Layer 21 Base: ['pound', 'pounds', 'currency', '£', 'Po']\n",
      "   - 🔸 Layer 21 LoRA: ['pound', 'pounds', 'Po', '£', '-']\n",
      "   - 🔹 Layer 22 Base: ['pound', 'pounds', 'Po', '£', 'currency']\n",
      "   - 🔸 Layer 22 LoRA: ['pound', 'pounds', 'Po', '£', '-']\n",
      "   - 🔹 Layer 23 Base: ['pound', 'pounds', 'Po', '-', 'British']\n",
      "   - 🔸 Layer 23 LoRA: ['pounds', 'Po', 'pound', '-', '£']\n",
      "   - 🔹 Layer 24 Base: ['Po', 'pound', 'pounds', '-', 'and']\n",
      "   - 🔸 Layer 24 LoRA: ['Po', 'pounds', 'pound', '-', 'British']\n",
      "   - 🔹 Layer 25 Base: ['pound', 'Po', 'pounds', 'L', 'currency']\n",
      "   - 🔸 Layer 25 LoRA: ['Po', 'pound', 'pounds', 'L', 'po']\n",
      "\n",
      "🟢 Step 4:\n",
      "   - 🔹 Layer 1 Base: ['pound', 'pounds', 'dollar', '£', 'oun']\n",
      "   - 🔸 Layer 1 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 2 Base: ['pound', 'pounds', 'dollar', '£', 'oun']\n",
      "   - 🔸 Layer 2 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 3 Base: ['pound', 'pounds', 'dollar', 'peso', 'pond']\n",
      "   - 🔸 Layer 3 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 4 Base: ['pound', 'pounds', 'dollar', 'coin', 'pond']\n",
      "   - 🔸 Layer 4 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 5 Base: ['pound', 'pounds', 'dollar', 'weight', 'coin']\n",
      "   - 🔸 Layer 5 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 6 Base: ['pound', 'pounds', 'dollar', 'square', 'coin']\n",
      "   - 🔸 Layer 6 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 7 Base: ['pound', 'pounds', 'dollar', 'coin', 'square']\n",
      "   - 🔸 Layer 7 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 8 Base: ['pound', 'coin', 'pounds', 'dollar', 'weight']\n",
      "   - 🔸 Layer 8 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 9 Base: ['pound', 'coin', 'pounds', 'weight', 'dollar']\n",
      "   - 🔸 Layer 9 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 10 Base: ['pound', 'coin', 'weight', 'pounds', 'dollar']\n",
      "   - 🔸 Layer 10 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 11 Base: ['pound', 'coin', 'ster', 'dollar', 'currency']\n",
      "   - 🔸 Layer 11 LoRA: ['Po', 'po', 'Po', 'po', 'PO']\n",
      "   - 🔹 Layer 12 Base: ['pound', 'coin', 'ster', 'currency', 'weight']\n",
      "   - 🔸 Layer 12 LoRA: ['Po', 'Po', 'po', 'po', 'cket']\n",
      "   - 🔹 Layer 13 Base: ['pound', 'coin', 'ster', 'weight', '(']\n",
      "   - 🔸 Layer 13 LoRA: ['Po', 'Po', 'po', 'po', 'cket']\n",
      "   - 🔹 Layer 14 Base: ['pound', 'coin', 'ster', '(', 'weight']\n",
      "   - 🔸 Layer 14 LoRA: ['Po', 'po', 'Po', 'cket', 'po']\n",
      "   - 🔹 Layer 15 Base: ['coin', 'pound', 'ster', '(', 'weight']\n",
      "   - 🔸 Layer 15 LoRA: ['Po', 'cket', 'po', 'Po', 'unds']\n",
      "   - 🔹 Layer 16 Base: ['coin', 'weight', 'pound', '(', 'ster']\n",
      "   - 🔸 Layer 16 LoRA: ['Po', 'cket', 'wers', 'po', 'unds']\n",
      "   - 🔹 Layer 17 Base: ['(', 'weight', 'coin', '-', 'ster']\n",
      "   - 🔸 Layer 17 LoRA: ['Po', 'cket', 'unds', 'wers', 'ints']\n",
      "   - 🔹 Layer 18 Base: ['(', 'coin', 'weight', '</s>', '-']\n",
      "   - 🔸 Layer 18 LoRA: ['Po', 'unds', 'ints', 'wers', 'cket']\n",
      "   - 🔹 Layer 19 Base: ['(', '</s>', 'coin', 'weight', '-']\n",
      "   - 🔸 Layer 19 LoRA: ['unds', 'ints', 'und', 'cket', 'wers']\n",
      "   - 🔹 Layer 20 Base: ['</s>', '(', 'coin', '-', 'and']\n",
      "   - 🔸 Layer 20 LoRA: ['unds', 'und', 'ints', 'cket', 'wers']\n",
      "   - 🔹 Layer 21 Base: ['</s>', '(', 'and', '-', 'coin']\n",
      "   - 🔸 Layer 21 LoRA: ['unds', 'und', 'ints', 'under', 'unding']\n",
      "   - 🔹 Layer 22 Base: ['</s>', '(', 'and', '-', 'is']\n",
      "   - 🔸 Layer 22 LoRA: ['und', 'unds', 'ints', 'under', 'unders']\n",
      "   - 🔹 Layer 23 Base: ['</s>', '(', '-', 'and', 'is']\n",
      "   - 🔸 Layer 23 LoRA: ['und', 'unds', 'ints', 'under', 'unt']\n",
      "   - 🔹 Layer 24 Base: ['</s>', '(', '-', 'and', 'is']\n",
      "   - 🔸 Layer 24 LoRA: ['und', 'unds', 'unt', 'ints', 'un']\n",
      "   - 🔹 Layer 25 Base: ['</s>', 'ster', 'or', 'and', ',']\n",
      "   - 🔸 Layer 25 LoRA: ['und', 'unds', 'under', 'unders', 'unt']\n"
     ]
    }
   ],
   "source": [
    "# Sample input\n",
    "input_text = \"британийн фунт\"\n",
    "\n",
    "# Extract hidden states for both models while preserving autoregressive behavior\n",
    "base_hidden_states, base_generated_tokens = base_model_analyzer.extract_logits_autoregressive(input_text)\n",
    "lora_hidden_states, lora_generated_tokens = lora_model_analyzer.extract_logits_autoregressive(input_text)\n",
    "\n",
    "# Compare models\n",
    "similarities, entropies, base_top_k, lora_top_k = base_model_analyzer.compare_models(base_hidden_states, lora_hidden_states)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0000001192092896,\n",
       " 0.9999422430992126,\n",
       " 0.9999804496765137,\n",
       " 0.9999713897705078,\n",
       " 0.9999773502349854,\n",
       " 0.9999727010726929,\n",
       " 0.9999706149101257,\n",
       " 0.9999663829803467,\n",
       " 0.9999510049819946,\n",
       " 0.9999452233314514,\n",
       " 0.9999325275421143,\n",
       " 0.9999133944511414,\n",
       " 0.9999029636383057,\n",
       " 0.9998898506164551,\n",
       " 0.9998372197151184,\n",
       " 0.9997361302375793,\n",
       " 0.9988932609558105,\n",
       " 0.9970133304595947,\n",
       " 0.9954288601875305,\n",
       " 0.9949801564216614,\n",
       " 0.993058979511261,\n",
       " 0.989745020866394,\n",
       " 0.983683705329895,\n",
       " 0.9715161919593811,\n",
       " 0.7704209685325623]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ys', 'YS', 'yst', 'yd', 'yse'],\n",
       " ['ys', 'YS', 'yd', 'yn', 'yst'],\n",
       " ['ys', 'yd', 'yst', 'yn', 'YS'],\n",
       " ['ys', 'ays', 'ying', 'yl', 'yst'],\n",
       " ['ys', 'laid', 'lay', 'ays', 'ying'],\n",
       " ['ys', 'laid', 'lay', 'Lay', 'lies'],\n",
       " ['laid', 'ys', 'lay', 'Lay', 'play'],\n",
       " ['laid', 'lay', 'ys', 'play', 'Lay'],\n",
       " ['laid', 'lay', 'down', 'Lay', 'ys'],\n",
       " ['laid', 'lay', 'down', 'around', 'Lay'],\n",
       " ['laid', 'lay', 'down', 'around', 'up'],\n",
       " ['up', 'around', 'down', 'lay', 'approximately'],\n",
       " ['up', 'only', 'out', 'approximately', 'around'],\n",
       " ['up', 'about', 'approximately', 'around', 'out'],\n",
       " ['up', 'about', 'approximately', 'more', 'around'],\n",
       " ['up', 'about', 'approximately', 'around', 'more'],\n",
       " ['up', 'about', 'approximately', 'only', 'more'],\n",
       " ['up', 'about', 'approximately', 'more', 'nearly'],\n",
       " ['up', 'about', 'approximately', 'more', 'around'],\n",
       " ['up', 'about', 'approximately', 'more', 'nearly'],\n",
       " ['up', 'about', 'a', 'approximately', '16'],\n",
       " ['up', 'about', '16', 'a', 'approximately'],\n",
       " ['up', '16', 'about', 'a', 'as'],\n",
       " ['16', 'up', 'a', 'about', 'as'],\n",
       " ['16', 'up', 'about', 'six', 'an']]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_top_k[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[362], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlora_top_k\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lora_top_k[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
