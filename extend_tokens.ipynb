{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set new cache directories\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/ephemeral/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/ephemeral/transformers_cache\"\n",
    "os.environ[\"TMPDIR\"] = \"/ephemeral/tmp\"\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(\"/ephemeral/hf_cache\", exist_ok=True)\n",
    "os.makedirs(\"/ephemeral/transformers_cache\", exist_ok=True)\n",
    "os.makedirs(\"/ephemeral/tmp\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original SentencePiece Vocabulary Size: 32000\n",
      "Tokenizer saved to /workspace/mongolian_llama_tokenizer\n",
      "Encoded text: {'input_ids': [1, 63, 4, 36, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "Decoded text: <s> сайн байна уу<unk>\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Load SentencePiece model\n",
    "# -----------------------------\n",
    "sentencepiece_model_path = \"/workspace/mongolian_tokenizer.model\"\n",
    "output_dir = \"/workspace/mongolian_llama_tokenizer\"  # Directory to save Hugging Face tokenizer\n",
    "\n",
    "# Check the SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file=sentencepiece_model_path)\n",
    "print(\"Original SentencePiece Vocabulary Size:\", sp.vocab_size())\n",
    "\n",
    "# -----------------------------\n",
    "# Wrap into Hugging Face format\n",
    "# -----------------------------\n",
    "# Use LlamaTokenizer or PreTrainedTokenizerFast\n",
    "tokenizer = LlamaTokenizer(vocab_file=sentencepiece_model_path)\n",
    "\n",
    "# -----------------------------\n",
    "# Add Special Tokens\n",
    "# -----------------------------\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"unk_token\": \"<unk>\"\n",
    "})\n",
    "\n",
    "# Save the tokenizer in Hugging Face format\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Tokenizer saved to {output_dir}\")\n",
    "\n",
    "# Test the tokenizer\n",
    "text = \"сайн байна уу?\"\n",
    "encoded = tokenizer(text)\n",
    "print(\"Encoded text:\", encoded)\n",
    "decoded = tokenizer.decode(encoded[\"input_ids\"])\n",
    "print(\"Decoded text:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94b9a2c94ea44d380ebc1d96ef12324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa49fc68e88b45f09d514bdf62bad4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Billyyy/mongolian-tokenizer-unigram/commit/b54741a2e0308d38ba637879dc01bff916b3b737', commit_message='Upload tokenizer', commit_description='', oid='b54741a2e0308d38ba637879dc01bff916b3b737', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Billyyy/mongolian-tokenizer-unigram', endpoint='https://huggingface.co', repo_type='model', repo_id='Billyyy/mongolian-tokenizer-unigram'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from transformers import LlamaTokenizer, LlamaTokenizerFast\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "tokenizer.push_to_hub(\"Billyyy/mongolian-tokenizer-unigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "source_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "source_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Load the SentencePiece tokenizer\n",
    "sentencepiece_model_path = \"/workspace/mongolian_tokenizer.model\"\n",
    "sp = spm.SentencePieceProcessor(model_file=sentencepiece_model_path)\n",
    "\n",
    "# Optional: Check existing vocab size\n",
    "print(\"Vocabulary size:\", sp.vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new tokens to add: 32000\n",
      "Sample new tokens: ['<unk>', '<s>', '</s>', '▁нь', '▁байна', '▁юм', '▁энэ', '▁', '▁ч', '▁байгаа']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract new tokens from SentencePiece tokenizer\n",
    "sentencepiece_vocab = [sp.id_to_piece(i) for i in range(sp.vocab_size())]\n",
    "\n",
    "# Compare with base tokenizer's vocabulary\n",
    "existing_vocab = tokenizer.get_vocab()\n",
    "new_tokens = [token for token in sentencepiece_vocab if token not in existing_vocab]\n",
    "\n",
    "print(f\"Number of new tokens to add: {len(new_tokens)}\")\n",
    "print(\"Sample new tokens:\", new_tokens[:10])\n",
    "\n",
    "# Add new tokens to the base tokenizer\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160256"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FOCUS to align embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOCUS initialization...: 100%|██████████| 31384/31384 [00:19<00:00, 1578.85it/s]                                     \n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOCUS alignment complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from deepfocus import FOCUS  # Ensure `deepfocus` library is installed\n",
    "import sentencepiece as spm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "extend_tokenizer = AutoTokenizer.from_pretrained(\"Billyyy/mongolian-tokenizer-unigram\")\n",
    "\n",
    "print(\"Running FOCUS to align embeddings...\")\n",
    "target_embeddings = FOCUS(\n",
    "    source_embeddings=source_model.get_input_embeddings().weight,\n",
    "    source_tokenizer=source_tokenizer,\n",
    "    target_tokenizer=target_tokenizer,\n",
    "    target_training_data_path=\"/ephemeral/cc100_mn_cleaned.txt\",\n",
    "    fasttext_model_dim=2048,\n",
    "    device = \"cpu\",\n",
    "    extend_tokenizer = extend_tokenizer,\n",
    "    processes = 27,\n",
    "    fasttext_model_epochs = 1,\n",
    "    verbosity=\"silent\"\n",
    "      # Path to your data\n",
    "    # Optional: Provide pre-trained FastText model for better alignment\n",
    "    # fasttext_model_path=\"/path/to/fasttext.bin\",\n",
    ")\n",
    "\n",
    "# Update input embeddings with aligned embeddings\n",
    "source_model.resize_token_embeddings(len(target_tokenizer))\n",
    "source_model.get_input_embeddings().weight.data = target_embeddings\n",
    "\n",
    "print(\"FOCUS alignment complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ec4e025d59414f801d7361125819e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/23.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b3feadfad142f583edbdc5daccff63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6206d6807dc4edcb0e715b68f150481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9914bdbceb4f67855ccc2d0d4ca4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81815dcf892f456bbf98b528b0d5026c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Billyyy/extended_llama_mongolian/commit/5f5a4aab00bc1cc699a472777ff0e7605c2a2589', commit_message='Upload LlamaForCausalLM', commit_description='', oid='5f5a4aab00bc1cc699a472777ff0e7605c2a2589', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Billyyy/extended_llama_mongolian', endpoint='https://huggingface.co', repo_type='model', repo_id='Billyyy/extended_llama_mongolian'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from transformers import LlamaTokenizer, LlamaTokenizerFast\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "target_tokenizer.push_to_hub(\"Billyyy/extended_llama_mongolian\")\n",
    "source_model.push_to_hub(\"Billyyy/extended_llama_mongolian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
